# Test Ollama - R√©sultats

## üîç V√©rification effectu√©e

### √âtat d'Ollama
- ‚ùì **Ollama** : Commandes de test ne retournent pas de r√©sultat
- Cela indique g√©n√©ralement qu'**Ollama n'est pas install√©** ou **pas en cours d'ex√©cution**

### Configuration actuelle
‚úÖ Le serveur proxy est **configur√© pour Ollama** :
- `USE_OLLAMA=true` dans `server/.env`
- `OLLAMA_HOST=http://localhost:11434`
- `OLLAMA_MODEL=llama3`

### Code serveur
‚úÖ Le code du proxy **supporte Ollama** nativement :
- D√©tecte automatiquement si le mod√®le commence par `ollama:`
- Transmet les requ√™tes √† `http://localhost:11434/api/chat`
- Formate correctement les r√©ponses

## üöÄ Pour activer la g√©n√©ration avec Ollama

### 1. Installer Ollama
```bash
# Windows
winget install Ollama.Ollama
# Ou t√©l√©charger depuis: https://ollama.ai/download
```

### 2. D√©marrer Ollama
```bash
ollama serve
```

### 3. T√©l√©charger un mod√®le
```bash
ollama pull llama3
# Ou un autre mod√®le: llama3:70b, mistral, codellama, etc.
```

### 4. V√©rifier que √ßa fonctionne
```bash
# Tester Ollama directement
ollama run llama3 "Bonjour, qui es-tu ?"

# Tester via notre proxy
node test-ollama.js
```

### 5. Utiliser dans l'app
1. Ouvrir http://localhost:8000/index.html
2. Se connecter
3. Dans le s√©lecteur de mod√®le (en haut), choisir **"Ollama (llama3)"**
4. Cr√©er un document et g√©n√©rer

## üîÑ Alternative : Utiliser OpenAI

Si vous pr√©f√©rez utiliser OpenAI au lieu d'Ollama :

1. Obtenir une cl√© API sur https://platform.openai.com/api-keys
2. √âditer `server/.env` :
   ```env
   OPENAI_API_KEY=sk-proj-votre-vraie-cle-ici
   USE_OLLAMA=false
   ```
3. Red√©marrer le serveur : `cd server && npm start`
4. Dans l'app, s√©lectionner **"OpenAI (gpt-3.5-turbo)"**

## üìä Comparaison

| Crit√®re | Ollama | OpenAI |
|---------|--------|--------|
| **Co√ªt** | Gratuit | Payant (~$0.50/1M tokens) |
| **Vitesse** | D√©pend du PC | Rapide (cloud) |
| **Confidentialit√©** | Local, priv√© | Donn√©es envoy√©es au cloud |
| **Setup** | Installation requise | Cl√© API uniquement |
| **Mod√®les** | llama3, mistral, etc. | gpt-3.5-turbo, gpt-4, etc. |

## ‚úÖ Conclusion

**Le code est pr√™t pour Ollama**, il suffit de :
1. Installer Ollama
2. Lancer `ollama serve`
3. Pull un mod√®le (`ollama pull llama3`)
4. Relancer le test `node test-ollama.js`

Tout est configur√© c√¥t√© code ! üéâ
