# Guide d'Installation Ollama - Windows

## ‚úÖ Checklist Installation

### √âtape 1 : T√©l√©charger Ollama
1. Visite https://ollama.ai/download
2. Clique sur **"Download for Windows"**
3. Attends la fin du t√©l√©chargement (~150 MB)

### √âtape 2 : Installer Ollama
1. Lance le fichier `OllamaSetup.exe`
2. Accepte les conditions
3. S√©lectionne le dossier d'installation (d√©faut: `C:\Users\<username>\AppData\Local\Programs\Ollama`)
4. Attends la fin de l'installation (~2 minutes)
5. Ollama se lancera automatiquement

### √âtape 3 : V√©rifier l'installation
1. Ouvre PowerShell ou Command Prompt
2. Tape : `ollama --version`
3. Tu dois voir la version (ex: `ollama version 0.1.15`)

Si tu vois "commande non reconnue", ajoute Ollama au PATH :
- Red√©marre ton ordinateur (l'installeur ajoute Ollama au PATH au red√©marrage)
- Ou ajoute manuellement `C:\Users\<username>\AppData\Local\Programs\Ollama` au PATH syst√®me

### √âtape 4 : T√©l√©charger un mod√®le
Une fois Ollama install√© :

```powershell
# Ouvrir une PowerShell en Admin
ollama pull llama3
```

Cela va t√©l√©charger le mod√®le llama3 (~4.7 GB) - cela peut prendre 10-30 minutes selon ta connexion.

### √âtape 5 : Lancer Ollama
```powershell
ollama serve
```

Tu dois voir :
```
Ollama is running at http://localhost:11434
```

Garde ce terminal ouvert ! (C'est le serveur Ollama)

### √âtape 6 : Tester Ollama
Dans un **autre terminal** :

```bash
# Test simple
ollama run llama3 "Bonjour, qui es-tu?"

# Ou test via notre proxy
node test-ollama.js
```

Tu dois voir une r√©ponse du mod√®le llama3.

### √âtape 7 : Tester avec SamaTutor
1. Ouvre http://localhost:8000/index.html
2. Connecte-toi (email + nom)
3. S√©lectionne **"Ollama (llama3)"** en haut
4. Cr√©e un document et clique **"G√©n√©rer avec l'IA"**

## üîß Configuration pour SamaTutor

Si tu as suivi les √©tapes, tout fonctionne automatiquement :
- ‚úÖ `server/.env` est configur√© avec `USE_OLLAMA=true`
- ‚úÖ Le proxy d√©tecte automatiquement les mod√®les `ollama:*`
- ‚úÖ Le frontend envoie au mod√®le Ollama

## ‚ùì D√©pannage

### "ollama: commande non reconnue"
‚Üí Red√©marre l'ordinateur, puis r√©essaie

### Ollama s'arr√™te apr√®s quelques secondes
‚Üí V√©rifier les fichiers journaux :
```
C:\Users\<username>\AppData\Local\Ollama\logs
```

### La g√©n√©ration est tr√®s lente
‚Üí C'est normal ! llama3 sur un PC standard prend 30-60 secondes par r√©ponse
‚Üí Pour plus rapide, utilise `ollama pull mistral` (plus petit/rapide)

### "Erreur: Impossible de se connecter √† Ollama"
‚Üí Assure-toi que `ollama serve` tourne
‚Üí V√©rifie que le port 11434 est libre : `netstat -ano | findstr :11434`

## üìä Mod√®les Disponibles

```bash
ollama pull llama3        # Excellent (4.7 GB, ~30-60s/r√©ponse)
ollama pull mistral       # Rapide (5.1 GB, ~15-30s/r√©ponse)
ollama pull neural-chat   # L√©ger (3.8 GB, ~10-20s/r√©ponse)
ollama pull codellama     # Code (3.6 GB, sp√©cialis√© code)
```

## ‚ú® R√©sum√© des Commandes

```bash
# V√©rifier l'installation
ollama --version

# Lancer le serveur (√† garder actif)
ollama serve

# T√©l√©charger un mod√®le
ollama pull llama3

# Lister les mod√®les install√©s
ollama list

# Tester un mod√®le
ollama run llama3 "ta question"

# Tester via notre proxy
node test-ollama.js
```

## üéâ Une fois tout install√©

1. Lance `ollama serve` dans un terminal
2. Lance le proxy serveur : `cd server && npm start`
3. Lance le frontend : `python -m http.server 8000`
4. Ouvre http://localhost:8000/index.html
5. Profite ! üöÄ

Questions ? Consulte https://github.com/ollama/ollama ou les fichiers PRODUCTION.md et OLLAMA-TEST.md
